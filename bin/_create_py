NUM_CORE_INSTANCE="${NUM_CORE_INSTANCE:-1}"
CORE_EBS_SIZE="${CORE_EBS_SIZE:-100}"

# create cluster
aws emr create-cluster --name "$CLUSTER_NAME" --region $AWS_REGION \
    --release-label $AWS_EMR_LABEL \
    --applications Name=Ganglia Name=Spark \
    --service-role EMR_DefaultRole \
    --instance-groups \
    InstanceGroupType=MASTER,InstanceType=$EC2_TYPE,InstanceCount=1 \
    "InstanceGroupType=CORE,InstanceType=$EC2_TYPE,InstanceCount=$NUM_CORE_INSTANCE,EbsConfiguration={EbsOptimized=true,EbsBlockDeviceConfigs=[{VolumeSpecification={VolumeType=gp2,SizeInGB=$CORE_EBS_SIZE}}]}" \
    InstanceGroupType=TASK,InstanceType=$EC2_TYPE,InstanceCount=$NUM_TASK_INSTANCE,BidPrice=$TASK_SPOT_BID_PRICE \
    --configurations '[{"Classification":"spark","Properties":{"maximizeResourceAllocation":"true"},"Configurations":[]}]' \
    --bootstrap-actions Name=Init_for_Wadal,Path=$INIT_ASSET_DIR_S3/init_py.sh \
    --ec2-attributes \
KeyName=$EC2_KEY_PAIR_NAME,\
InstanceProfile=EMR_EC2_DefaultRole,\
SubnetId=$AWS_EMR_SUBNET\
    --steps Type=CUSTOM_JAR,Name=CopyAssets,ActionOnFailure=CANCEL_AND_WAIT,Jar=s3://$AWS_REGION.elasticmapreduce/libs/script-runner/script-runner.jar,Args=["$INIT_ASSET_DIR_S3/cp_assets.sh"] Type=CUSTOM_JAR,Name=RunJupyter,ActionOnFailure=CANCEL_AND_WAIT,Jar=s3://$AWS_REGION.elasticmapreduce/libs/script-runner/script-runner.jar,Args=["$INIT_ASSET_DIR_S3/run_jupyter.sh","$AWS_S3_ACCESS_KEY","$AWS_S3_SECRET_KEY","$NOTEBOOK_S3_BUCKET"] | sed -n '/ClusterId/p' | cut -d':' -f2 | tr -d ' "' > "clusters/$1_cluster.id"
